{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d625e0",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Стохастический градиентный спуск</h1>\n",
    "\n",
    "<h1 style=\"color:#008B8B\">1. Градиентный спуск</h1>\n",
    "\n",
    "Нейронные сети являются дифференцируемыми моделями, значит мы можем посчитать производные по всем параметрам, а если мы можем посчитать произвдные, значит возможно использование градиентных методов обучения, в частности градиентный спуск.\n",
    "\n",
    "**Алгоритм:**\n",
    "\n",
    "1. Начальное приближение $w^0$\n",
    "\n",
    "2. Повторять:\n",
    "\n",
    "$$\\large w^t = w^{t-1} - \\eta \\nabla Q(w^{t-1})$$\n",
    "\n",
    "3. Останавливаемся, если $\\| w^t - w^{t-1}\\| < \\epsilon$ В случае нейронных сетей, данных подход имеет ряд недостатков. Небходимо хранить два набора весов, когда их у нейронной сети много. \n",
    "\n",
    "    * Обычно сравнивают качество по отложенной выборке, останавливаемся, если ошибка на тестовой выборке перестаёт убывать.\n",
    "\n",
    "### Линейная регрессия\n",
    "\n",
    "$$\\large Q(w) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (\\langle w, x_i \\rangle - y_i)^2$$\n",
    "\n",
    "Считаем частные производные по всем параметрам, кторых $d$\n",
    "\n",
    "* $\\large \\frac{\\partial Q}{\\partial w_1} = \\frac{2}{\\ell} \\sum\\limits_{i=1}^{\\ell} x_{i, 1} (\\langle w, x_i \\rangle - y_i)$\n",
    "\n",
    "* $\\large \\ldots$\n",
    "\n",
    "* $\\large \\frac{\\partial Q}{\\partial w_d} = \\frac{2}{\\ell} \\sum\\limits_{i=1}^{\\ell} x_{i, d} (\\langle w, x_i \\rangle - y_i)$\n",
    "\n",
    "### Сложность гадиентного спуска\n",
    "\n",
    "* Для вычисления градиента, как правило, надо просуммировать что-то по всем объектам\n",
    "* И это для одного маленького шага!\n",
    "\n",
    "### Оценка градиента\n",
    "\n",
    "$$\\large Q(w) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} L(y_i, a(x_i))$$\n",
    "\n",
    "* Градиент:\n",
    "\n",
    "$$\\large \\nabla Q(w) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} \\nabla L(y_i, a(x_i))$$\n",
    "\n",
    "* Может, оценить градиент одним слогаемым?\n",
    "\n",
    "$$\\large \\nabla Q(w) \\approx \\nabla L(y_i, a(x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46318445",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">2. Стохастический градиентный спуск</h1>\n",
    "\n",
    "**Алгоритм:**\n",
    "\n",
    "1. Начальное приближение $w^0$\n",
    "\n",
    "2. Повторять, каждый раз выбирая случайный объект $i_t$\n",
    "\n",
    "$$\\large w^t = w^{t-1} - \\eta_t \\nabla L(y_{i_t}, a(x_{i_t}))$$\n",
    "\n",
    "3. Останавливаемся, если ошибка на тестовой выборке перестаёт убывать.\n",
    "\n",
    "**О стохастическом градиентном спуске:**\n",
    "\n",
    "* Оценка по одному объекту несмещённая\n",
    "* То есть в среднем мы идём в правильную сторону\n",
    "* Даже в точке оптимума оценка по одному объекту вряд ли будет нулевой\n",
    "* Поэтому важно, чтобы длина шага стремилась к нулю\n",
    "* Сходимость к глобальному минимуму гарантируется только для выпуклых функций\n",
    "\n",
    "\n",
    "### Mini-batch GD\n",
    "\n",
    "**Алгоритм:**\n",
    "\n",
    "1. Начальное приближение $w^0$\n",
    "\n",
    "2. Повторять, каждый раз выбирая $m$ случайных объектов $i_1, \\ldots, i_m$\n",
    "\n",
    "$$\\large w^t = w^{t-1} - \\eta_t \\frac{1}{n} \\sum\\limits_{j=1}^{n}  \\nabla L(y_{t, j}, a(x_{t, j}))$$\n",
    "\n",
    "3. Останавливаемся, если ошибка на тестовой выборке перестаёт убывать.\n",
    "\n",
    "$x_{t, j}$ — объект номер $j$ из батча, сформированного на шаге $t$\n",
    "\n",
    "**Batch size**\n",
    "\n",
    "* Размер пакета — обычно порядка десятков или сотни\n",
    "* Имеет смысл брать степень двойки\n",
    "* Возможно, делает оценку градиента более стабильной\n",
    "* Вычислительно почти так же эффективен, как шаг по градиенту одного объекта — за счёт векторизации\n",
    "\n",
    "**Размер батча 1.**\n",
    "\n",
    "При увеличении батча происходит просадка качества. Если для каждого батча подбирать стратегию изменения длины шага, тогда можно обучаться с батчами больше и мы не будем терять в качестве.\n",
    "\n",
    "https://arxiv.org/pdf/1706.02677.pdf\n",
    "\n",
    "**Размер батча 2.**\n",
    "\n",
    "Оказывается, если зафиксировать длину шага, но при этом, по мере эпох увеличивать размер батча (по определенной стратегии). Тогда обучение нейронных сетей будет эффективным и мы быстрее достигнем того же качества, нежели при фиксированном батче и убываешем шаге. \n",
    "\n",
    "https://arxiv.org/pdf/1711.00489.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc903f45",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Модификации градиентного спуска </h1>\n",
    "\n",
    "<h1 style=\"color:#008B8B\">1. Momentum</h1>\n",
    "\n",
    "Решает проблему\n",
    "\n",
    "* Если у функции «вытянуты» линии уровня, то градиентный спуск требует аккуратного выбора длины шага и будет долго сходиться\n",
    "\n",
    "**Алгоритм:**\n",
    "\n",
    "$$\\large h_t = \\alpha h_{t-1} + \\eta_t \\nabla Q(w^{t-1})$$\n",
    "\n",
    "$$\\large w^t = w^{t-1} - h_t$$\n",
    "\n",
    "* $h_t$ — «инерция», усреднённое направление движения\n",
    "* $\\alpha$ — параметр затухания\n",
    "\n",
    "* Как будто шарик, который катится в сторону минимума, очень тяжёлый\n",
    "\n",
    "### Nesterov Momentum\n",
    "\n",
    "$$\\large h_t = \\alpha h_{t-1} + \\eta_t \\nabla Q(w^{t-1} - \\alpha h_{t-1})$$\n",
    "\n",
    "$$\\large w^t = w^{t-1} - h_t$$\n",
    "\n",
    "* $w^{t-1} - \\alpha h_{t-1}$ — неплохая оценка того, куда мы попадём на\n",
    "следующем шаге"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b21bd",
   "metadata": {},
   "source": [
    "## Имеются следующие проблемы\n",
    "\n",
    "**Проблема с разреженными данными**\n",
    "\n",
    "* По разным параметрам мы движемся с разной скоростью\n",
    "*  Будет здорово это учитывать — иначе мы обучим разные параметры с разным качеством\n",
    "\n",
    "**Проблема с разными масштабами**\n",
    "\n",
    "* Допустим, признаки имеют разный масштаб — от единиц до миллионов\n",
    "* Тогда странно шагать по каждому параметру с одинаковой скоростью\n",
    "\n",
    "<h1 style=\"color:#008B8B\">2. AdaGrad</h1>\n",
    "\n",
    "**Алгоритм**\n",
    "\n",
    "$\\large G_{j}^t = G_{j}^{t-1} + (\\nabla_w Q(w^{t-1}))_j^2$ \n",
    "\n",
    "Где $t$ - номер итерации, $j$ - номер признака. $G_{j}^t$ - эта переменна говорит, насколько к шагу номер $t$ сделано обновлений по $j$ параметру.\n",
    "\n",
    "Как обновить $G_{j}^t$? Берем значение параметра с прошлой итерации $G_{j}^{t-1}$ и добавляем квадрат градиента $j$ компонены $(\\nabla_w Q(w^{t-1}))_j^2$.\n",
    "\n",
    "$\\large w_j^{t} = w_j^{t-1} - \\frac{\\eta_t}{\\sqrt{G_{j}^t + \\epsilon}} (\\nabla_w Q(w^{t-1}))_j$\n",
    "\n",
    "Как обновляем $j$ параметр на шаге $t$? Берем значение $j$ параметра на прошлом шаге $w_j^{t-1}$ вычитаем из него градиент $j$ компонены $(\\nabla_w Q(w^{t-1}))_j$ умноженное на коэффициент $\\frac{\\eta_t}{\\sqrt{G_{j}^t + \\epsilon}}$ (чем больше мы нашогали по этому параметру, тем больше будет знаменатель и тем медленнее мы будем по нему шагать)\n",
    "\n",
    "* По каждому параметру своя скорость\n",
    "* $\\eta_n$ можно зафиксировать\n",
    "* Длина шага может убывать слишком быстро и привести к ранней остановке"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a8abc9",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "Проблема метода выше заключается в том, что $G_{j}^t$ всегда возрастает, тогда знаменатель монотонно возрастает, из-за чего мы можем слишком рано остановить обучение по некоторому параметру.\n",
    "\n",
    "В текущем методе мы добавляем затухание, забывая ифнормацию о прошлом движении и это решает проблему в знаменателе.\n",
    "\n",
    "**Алгоритм:**\n",
    "\n",
    "$\\large G_{j}^k = \\alpha G_{j}^{k-1} + (1 - \\alpha) (\\nabla_w Q(w^{k-1}))_j^2$\n",
    "\n",
    "$\\large w_j^{t} = w_j^{t-1} - \\frac{\\eta_t}{\\sqrt{G_{j}^t + \\epsilon}} (\\nabla_w Q(w^{t-1}))_j$\n",
    "\n",
    "* $\\alpha$ можно взять около $0.9$\n",
    "* Скорость зависит только от недавних шагов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ff9633",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">3. Adam</h1>\n",
    "\n",
    "Совмещает в себе идеи из метода инерции и RMSProp\n",
    "\n",
    "**Алгоритм**\n",
    "\n",
    "$\\large m_j^t \n",
    "= \n",
    "\\frac{\\beta_1 m_j^{t-1} + (1-\\beta_1) (\\nabla_w Q(w^{t-1}))_j}\n",
    "    {1 - \\beta_1^t}$\n",
    "\n",
    "$\\large v_j^t \n",
    "= \n",
    "\\frac{\\beta_2 v_j^{t-1} + (1-\\beta_2) (\\nabla_w Q(w^{t-1}))_j^2}\n",
    "    {1 - \\beta_2^t}$\n",
    "    \n",
    "$\\large w_j^t \n",
    "= \n",
    "w_j^{t-1} \n",
    "- \n",
    "\\frac{\\eta_t}\n",
    "    {\\sqrt{v_j^t} + \\epsilon} m_j^t$\n",
    "    \n",
    "* Рекомендации: $\\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 10^{-8}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d00d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
