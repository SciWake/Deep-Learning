{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb733aa8",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Обратное распространение ошибки</h1>\n",
    "\n",
    "<h1 style=\"color:#008B8B\">1 Обучение нейронных сетей</h1>\n",
    "\n",
    "* Все слои обычно дифференцируемы, поэтому можно посчитать производные по всем параметрам.\n",
    "\n",
    "<img src='img/lecture02/1.png'>\n",
    "\n",
    "* $\\large a(x) = \\text{FC}_2(f(\\text{FC}_1(x)))$\n",
    "\n",
    "Параметры содержатся внутри $\\text{FC}_1, \\text{FC}_2$, так как полносвязные слои это набор линейных моделей с некоторыми весами, которые необходимо обучить. Для обучения зписываем функционал ошибки и минимизируем его:\n",
    "\n",
    "$$\\large \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} L(y_i, a(x_i)) \\to \\underset{a}{\\text{min}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04103c76",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">1.1 Как считать производные?</h2>\n",
    "\n",
    "<img src='img/lecture02/2.png'>\n",
    "\n",
    "На вход подаются два признака $x_1, x_2$, мы видим три полносвязных слоя:\n",
    "\n",
    "* Первый полносвязный слой - с некоторыми весами суммируем входные признаки и получаем два выходных числа $z_1, z_2$.\n",
    "* Выходы первого слоя $z_1, z_2$ суммируем с некоторыми весами и получаем два выходных числа $h_1, h_2$.\n",
    "* После $h_1, h_2$ суммируем с некоторыми коэффициентами и получаем прогноз модели.\n",
    "\n",
    "### На примере\n",
    "\n",
    "Стоит заметить, что в нашем примере нет нелинейности, так как это усложняет подсчеты. Возьмем конкретные веса для нашей нейронной сети:\n",
    "\n",
    "<img src='img/lecture02/3.png'>\n",
    "\n",
    "Посчитаем для наших входов значение каждого узла:\n",
    "\n",
    "<img src='img/lecture02/4.png'>\n",
    "\n",
    "Что если мы изменим выделенный вес $1$? Прогноз модели не изменится, так как в 3 слое входное значение равено $0$. Производная это характеристика того, насколько будет изменятся функция если мы будем изменять вход. Производная по весу будет говорить о том, насколько сильно изменится прогноз модели, если мы изменим данный вес. В нашем случае, производная будет равняться нулю. Поэтому нам необходимо изменить другой вес, для второго слоя у нейрона изменим вес равный $0$ на $1$:\n",
    "\n",
    "<img src='img/lecture02/5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f0bc0",
   "metadata": {},
   "source": [
    "## Общий вид\n",
    "\n",
    "### Производная по $p_{11}$\n",
    "\n",
    "$$\\large a(x) = p_{11} h_1(x) + p_{21} h_2(x)$$\n",
    "\n",
    "<img src='img/lecture02/6.png'>\n",
    "\n",
    "Продифференцируем выход модели $a$ по весу $p_{11}$\n",
    "\n",
    "$$\\large \\frac{\\partial a}{\\partial p_{11}} = h_1(x)$$\n",
    "\n",
    "* Чем больше $h_1(x)$, тем сильнее $p_{11}$ влияет на $a$\n",
    "\n",
    "### Производная по $v_{11}$\n",
    "\n",
    "В данном случае, вместо $h_{1}$ мы подставляем чему равняется $h_1$\n",
    "\n",
    "$$\\large a(x) = p_{11} f(v_{11} z_1(x) + v_{21} z_2(x)) + p_{21} h_2(x)$$\n",
    "\n",
    "<img src='img/lecture02/7.png'>\n",
    "\n",
    "$$\\large \\frac{\\partial a}{\\partial v_{11}}  = \\frac{\\partial a}{\\partial h_{1}} \\frac{\\partial h_1}{\\partial v_{11}} = p_{11} \\cdot f^{'} z_{1}(x)$$\n",
    "\n",
    "Для нахождения производной модели по внутреннему весу, необходимо знать производную по более позднему слою."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c3b6a1",
   "metadata": {},
   "source": [
    "### Производная по $w_{11}$\n",
    "\n",
    "<img src='img/lecture02/8.png'>\n",
    "\n",
    "Как посчитать производную $a$ по $w_{11}$?\n",
    "\n",
    "$\\large \\frac{\\partial a}{\\partial w_{11}} = ?$\n",
    "\n",
    "Если мы запишем как зависит $a$ от $w_{11}$ и посчитаем производную, тогда это будет очень сложно. Воспользуемся другим подходом:\n",
    "\n",
    "* Как сильно изменяется $a$ при изменении $w_{11}$? Если мы будем изменять $v_{11}$, будет ли изменяться влияние на $w_{11}$ на выход? Конечно, так как $w_{11}$ влияет на $z_1$, а $z_{1}$ через $v_{11}$ влияет на выход.\n",
    "\n",
    "<img src='img/lecture02/9.png'>\n",
    "\n",
    "* Если мы будем изменять $v_{12}$, будет ли изменяться влияние на $w_{11}$ на выход? Будет, так как $w_{11}$ влияет на $z_1$, а $z_{1}$ через $v_{12}$ влияет на выход.\n",
    "\n",
    "<img src='img/lecture02/10.png'>\n",
    "\n",
    "* Если мы будем изменять $w_{22}$, будет ли изменяться влияние на $w_{11}$ на выход? Нет, так как $w_{11}$ влияет только $z_{1}$, то есть, $w_{22}$ не проходит через $w_{11}$.\n",
    "\n",
    "**Нахождение производной**\n",
    "\n",
    "<img src='img/lecture02/11.png'>\n",
    "\n",
    "Чтобы посчитать частную производную выхода по $w_{11}$, необходимо найти все пути, которые идут из $w_{11}$ в выход и просуммировать по каждому пути все частные производные:\n",
    "\n",
    "$$\\large \\frac{\\partial a}{\\partial w_{11}}  = \\frac{\\partial a}{\\partial h_{1}} \\frac{\\partial h_1}{\\partial z_{1}} \\frac{\\partial z_1}{\\partial w_{11}} +\\frac{\\partial a}{\\partial h_{2}} \\frac{\\partial h_2}{\\partial z_{1}} \\frac{\\partial z_1}{\\partial w_{11}}$$\n",
    "\n",
    "Так как $w_{11}$ влияет на $a$ через два пути, поэтому у нас два слогаемых. Вдоль каждого пути суммируем значения частных производных, производная $a$ по $h_{1}$, производная $h_1$ по $z_{1}$, производная $z_1$ по $w_{11}$ + ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff78fb6",
   "metadata": {},
   "source": [
    "## Метод обратного распространения ошибки\n",
    "\n",
    "Делаем вывод, чтобы посчитать производную по параметрам некоторого слоя, необходимо знать производные по параметрам более поздних слоев.\n",
    "\n",
    "<img src='img/lecture02/12.png'>\n",
    "\n",
    "* Мы как бы идём в обратную сторону по графу и считаем производные. Мы сначал считаем производные выхода модели по $h_1, h_2$, зная эти производные мы можем посчитать производные по параметра. Зная производные $z_1, z_2$, мы можем посчитать производные по следующим параметрам.\n",
    "\n",
    "* Метод обратного распространения ошибки (backpropagation)\n",
    "\n",
    "Рассмотрим производные выхода модели по выходам всех нейронов. Если мы хотим посчитать производную по входу, Тогда у нас будет 3 пути из $x_1$ в выход и по ним мы суммируем все частные производные. Но для нас нет смысла считать производную по значению признака, так как нам необходимо считать производные по тем параметрам, которые мы будем изменять. Ну тогда производная по значению признака не нужна, так как мы не будем изменять значение признака.\n",
    "\n",
    "<img src='img/lecture02/13.png'>\n",
    "\n",
    "**Заметки по Backprop:**\n",
    "\n",
    "* Во многие формулы входят одни и те же производные\n",
    "\n",
    "* В backprop каждая частная производная вычисляется один раз — вычисление производных по слою N сводится к перемножению матрицы производных по слою N+1 и некоторых векторов\n",
    "\n",
    "Чтобы посчитать производную по параметрам слоя - необходимо знать производные по всем следующим слоям. И если нам необходимо посчитать производные выхода модели по некоторому слою, тогда подсчет производных можно выразить как умножение матрицы произвдных выхода следующего слоя умноженное на некоторый вектор."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86890d8",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">2 Полносвязные сети для изображений</h1>\n",
    "\n",
    "Имеется рукопистный набор данных MNIST:\n",
    "\n",
    "* Изображения 28 x 28\n",
    "* Изображения центрированы\n",
    "* 60.000 объектов в обучающей выборке\n",
    "\n",
    "На вход нейронной сети будет подаваться картинка размером $28 x 28$ пикселей, значит всего 784 пикселя и запишим их как один вектор и будем подавать на вход нейронной сети. \n",
    " \n",
    "В первом полносвязном слоее мы имеем 10 нейронов, каждый нейрон - это сумма входных пикселей с некоторыми весами и получаем 10 новых чисел (нейронов). Дальше во втором полносвязном слое будет 10 нейронов и каждый отвечает за свой класс и дальше мы смотрим, в каком выходном слоее смотрим в каком нейроне получилось наибольшее число и делам вывод о том, какая цифра находится на картинке.\n",
    "\n",
    "<img src='img/lecture02/14.png'>\n",
    "\n",
    "И какой смысл получается у каждого нейрона? Нейрон суммирует с некоторыми весами все входные пиксили. Например, первый нейрон будет суммировать некотрые пиксили с нулевыми весами, так как некоторая область на картинке будет постоянно пустой - это те области, где ни разу не встречались черные пиксели для некоторой цифры. Соответсвенно, каждый нейрон может детектировать заполненность конкретного набора пикселей. После мы можем скомбинировать эти знания в следующем слоее и пполучим некоторый результат.\n",
    "\n",
    "### Проблема \n",
    "\n",
    "Мы можем научиться нейронами детектировать заполненность конкретных областей, но если мы детектируем 1, как заполненность области по центру, тогда что будет если мы напишем еденицу где-то сбоку? Тогда нейронная не сможет понять что это 1, так как мы детектируем черные пиксили по центру. То есть, если мы применяем полносвязные нейронные сети, тогда они будут выучить конкретное расположение объектов на картинке.\n",
    "\n",
    "* Если немного сдвинуть цифру, то нейрон уже не будет на неё реагировать.\n",
    "\n",
    "### Число параметров\n",
    "\n",
    "* Если у нас 784 входа\n",
    "* Полносвязный слой: 1000 нейронов\n",
    "* Выходной слой: 10 нейронов (по одному на каждый класс)\n",
    "* Весов между входным и полносвязным слоями: (784 + 1)*1000 = 785.000 параметров\n",
    "* Весов между полносвязным и выходным слоями: (1000 + 1) * 10 = 10.010 параметров\n",
    "\n",
    "И как мы знаем, что если параметров больше чем данных - то это плохо.\n",
    "\n",
    "### Выводы\n",
    "\n",
    "Использование полносвязных нейронных сетей для работы с картинками является плохой идей так как:\n",
    "\n",
    "* Очень много параметров\n",
    "* Легко могут переобучиться\n",
    "* Не учитывают специфику изображений (сдвиги, небольшие изменения формы и т.д.)\n",
    "* Один из лучших способов борьбы с переобучением — снижение числа параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ff118",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2aeb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
