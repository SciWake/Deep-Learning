{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb733aa8",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Обратное распространение ошибки</h1>\n",
    "\n",
    "<h1 style=\"color:#008B8B\">1 Обучение нейронных сетей</h1>\n",
    "\n",
    "* Все слои обычно дифференцируемы, поэтому можно посчитать производные по всем параметрам.\n",
    "\n",
    "<img src='img/lecture02/1.png'>\n",
    "\n",
    "* $\\large a(x) = \\text{FC}_2(f(\\text{FC}_1(x)))$\n",
    "\n",
    "Параметры содержатся внутри $\\text{FC}_1, \\text{FC}_2$, так как полносвязные слои это набор линейных моделей с некоторыми весами, которые необходимо обучить. Для обучения зписываем функционал ошибки и минимизируем его:\n",
    "\n",
    "$$\\large \\frac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} L(y_i, a(x_i)) \\to \\underset{a}{\\text{min}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04103c76",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">1.1 Как считать производные?</h2>\n",
    "\n",
    "<img src='img/lecture02/2.png'>\n",
    "\n",
    "На вход подаются два признака $x_1, x_2$, мы видим три полносвязных слоя:\n",
    "\n",
    "* Первый полносвязный слой - с некоторыми весами суммируем входные признаки и получаем два выходных числа $z_1, z_2$.\n",
    "* Выходы первого слоя $z_1, z_2$ суммируем с некоторыми весами и получаем два выходных числа $h_1, h_2$.\n",
    "* После $h_1, h_2$ суммируем с некоторыми коэффициентами и получаем прогноз модели.\n",
    "\n",
    "### На примере\n",
    "\n",
    "Стоит заметить, что в нашем примере нет нелинейности, так как это усложняет подсчеты. Возьмем конкретные веса для нашей нейронной сети:\n",
    "\n",
    "<img src='img/lecture02/3.png'>\n",
    "\n",
    "Посчитаем для наших входов значение каждого узла:\n",
    "\n",
    "<img src='img/lecture02/4.png'>\n",
    "\n",
    "Что если мы изменим выделенный вес $1$? Прогноз модели не изменится, так как в 3 слое входное значение равено $0$. Производная это характеристика того, насколько будет изменятся функция если мы будем изменять вход. Производная по весу будет говорить о том, насколько сильно изменится прогноз модели, если мы изменим данный вес. В нашем случае, производная будет равняться нулю. Поэтому нам необходимо изменить другой вес, для второго слоя у нейрона изменим вес равный $0$ на $1$:\n",
    "\n",
    "<img src='img/lecture02/5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a29dfd9",
   "metadata": {},
   "source": [
    "## Общий вид\n",
    "\n",
    "### Производная по $p_{11}$\n",
    "\n",
    "$$\\large a(x) = p_{11} h_1(x) + p_{21} h_2(x)$$\n",
    "\n",
    "<img src='img/lecture02/6.png'>\n",
    "\n",
    "Продифференцируем выход модели $a$ по весу $p_{11}$\n",
    "\n",
    "$$\\large \\frac{\\partial a}{\\partial p_{11}} = h_1(x)$$\n",
    "\n",
    "* Чем больше $h_1(x)$, тем сильнее $p_{11}$ влияет на $a$\n",
    "\n",
    "### Производная по $v_{11}$\n",
    "\n",
    "В данном случае, вместо $h_{1}$ мы подставляем чему равняется $h_1$\n",
    "\n",
    "$$\\large a(x) = p_{11} f(v_{11} z_1(x) + v_{21} z_2(x)) + p_{21} h_2(x)$$\n",
    "\n",
    "<img src='img/lecture02/7.png'>\n",
    "\n",
    "$$\\large \\frac{\\partial a}{\\partial v_{11}}  = \\frac{\\partial a}{\\partial h_{1}} \\frac{\\partial h_1}{\\partial v_{11}} = p_{11} \\cdot f^{'} z_{1}(x)$$\n",
    "\n",
    "Для нахождения производной модели по внутреннему весу, необходимо знать производную по более позднему слою."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e32044",
   "metadata": {},
   "source": [
    "### Производная по $w_{11}$\n",
    "\n",
    "<img src='img/lecture02/8.png'>\n",
    "\n",
    "Как посчитать производную $a$ по $w_{11}$?\n",
    "\n",
    "$\\large \\frac{\\partial a}{\\partial w_{11}} = ?$\n",
    "\n",
    "Если мы запишем как зависит $a$ от $w_{11}$ и посчитаем производную, тогда это будет очень сложно. Воспользуемся другим подходом:\n",
    "\n",
    "* Как сильно изменяется $a$ при изменении $w_{11}$? Если мы будем изменять $v_{11}$, будет ли изменяться влияние на $w_{11}$ на выход? Конечно, так как $w_{11}$ влияет на $z_1$, а $z_{1}$ через $v_{11}$ влияет на выход.\n",
    "\n",
    "<img src='img/lecture02/9.png'>\n",
    "\n",
    "* Если мы будем изменять $v_{12}$, будет ли изменяться влияние на $w_{11}$ на выход? Будет, так как $w_{11}$ влияет на $z_1$, а $z_{1}$ через $v_{12}$ влияет на выход.\n",
    "\n",
    "<img src='img/lecture02/10.png'>\n",
    "\n",
    "* Если мы будем изменять $w_{22}$, будет ли изменяться влияние на $w_{11}$ на выход? Нет, так как $w_{11}$ влияет только $z_{1}$, то есть, $w_{22}$ не проходит через $w_{11}$.\n",
    "\n",
    "**Нахождение производной**\n",
    "\n",
    "<img src='img/lecture02/11.png'>\n",
    "\n",
    "Чтобы посчитать частную производную выхода по $w_{11}$, необходимо найти все пути, которые идут из $w_{11}$ в выход и просуммировать по каждому пути все частные производные:\n",
    "\n",
    "$$\\large \\frac{\\partial a}{\\partial w_{11}}  = \\frac{\\partial a}{\\partial h_{1}} \\frac{\\partial h_1}{\\partial z_{1}} \\frac{\\partial z_1}{\\partial w_{11}} +\\frac{\\partial a}{\\partial h_{2}} \\frac{\\partial h_2}{\\partial z_{1}} \\frac{\\partial z_1}{\\partial w_{11}}$$\n",
    "\n",
    "Так как $w_{11}$ влияет на $a$ через два пути, поэтому у нас два слогаемых. Вдоль каждого пути суммируем значения частных производных, производная $a$ по $h_{1}$, производная $h_1$ по $z_{1}$, производная $z_1$ по $w_{11}$ + ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c985fb",
   "metadata": {},
   "source": [
    "## Метод обратного распространения ошибки\n",
    "\n",
    "Делаем вывод, чтобы посчитать производную по параметрам некоторого слоя, необходимо знать производные по параметрам более поздних слоев.\n",
    "\n",
    "<img src='img/lecture02/12.png'>\n",
    "\n",
    "* Мы как бы идём в обратную сторону по графу и считаем производные. Мы сначал считаем производные выхода модели по $h_1, h_2$, зная эти производные мы можем посчитать производные по параметра. Зная производные $z_1, z_2$, мы можем посчитать производные по следующим параметрам.\n",
    "\n",
    "* Метод обратного распространения ошибки (backpropagation)\n",
    "\n",
    "Рассмотрим производные выхода модели по выходам всех нейронов. Если мы хотим посчитать производную по входу, Тогда у нас будет 3 пути из $x_1$ в выход и по ним мы суммируем все частные производные. Но для нас нет смысла считать производную по значению признака, так как нам необходимо считать производные по тем параметрам, которые мы будем изменять. Ну тогда производная по значению признака не нужна, так как мы не будем изменять значение признака.\n",
    "\n",
    "<img src='img/lecture02/13.png'>\n",
    "\n",
    "**Заметки по Backprop:**\n",
    "\n",
    "* Во многие формулы входят одни и те же производные\n",
    "\n",
    "* В backprop каждая частная производная вычисляется один раз — вычисление производных по слою N сводится к перемножению матрицы производных по слою N+1 и некоторых векторов\n",
    "\n",
    "Чтобы посчитать производную по параметрам слоя - необходимо знать производные по всем следующим слоям. И если нам необходимо посчитать производные выхода модели по некоторому слою, тогда подсчет производных можно выразить как умножение матрицы произвдных выхода следующего слоя умноженное на некоторый вектор."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb7f7d9",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">2 Полносвязные сети для изображений</h1>\n",
    "\n",
    "Имеется рукопистный набор данных MNIST:\n",
    "\n",
    "* Изображения 28 x 28\n",
    "* Изображения центрированы\n",
    "* 60.000 объектов в обучающей выборке\n",
    "\n",
    "На вход нейронной сети будет подаваться картинка размером $28 x 28$ пикселей, значит всего 784 пикселя и запишим их как один вектор и будем подавать на вход нейронной сети. \n",
    " \n",
    "В первом полносвязном слоее мы имеем 10 нейронов, каждый нейрон - это сумма входных пикселей с некоторыми весами и получаем 10 новых чисел (нейронов). Дальше во втором полносвязном слое будет 10 нейронов и каждый отвечает за свой класс и дальше мы смотрим, в каком выходном слоее смотрим в каком нейроне получилось наибольшее число и делам вывод о том, какая цифра находится на картинке.\n",
    "\n",
    "<img src='img/lecture02/14.png'>\n",
    "\n",
    "И какой смысл получается у каждого нейрона? Нейрон суммирует с некоторыми весами все входные пиксили. Например, первый нейрон будет суммировать некотрые пиксили с нулевыми весами, так как некоторая область на картинке будет постоянно пустой - это те области, где ни разу не встречались черные пиксели для некоторой цифры. Соответсвенно, каждый нейрон может детектировать заполненность конкретного набора пикселей. После мы можем скомбинировать эти знания в следующем слоее и пполучим некоторый результат.\n",
    "\n",
    "### Проблема \n",
    "\n",
    "Мы можем научиться нейронами детектировать заполненность конкретных областей, но если мы детектируем 1, как заполненность области по центру, тогда что будет если мы напишем еденицу где-то сбоку? Тогда нейронная не сможет понять что это 1, так как мы детектируем черные пиксили по центру. То есть, если мы применяем полносвязные нейронные сети, тогда они будут выучить конкретное расположение объектов на картинке.\n",
    "\n",
    "* Если немного сдвинуть цифру, то нейрон уже не будет на неё реагировать.\n",
    "\n",
    "Подбробнее: https://srome.github.io/Jitter,-Convolutional-Neural-Networks,-and-a-Kaggle-Framework/\n",
    "\n",
    "### Число параметров\n",
    "\n",
    "* Если у нас 784 входа\n",
    "* Полносвязный слой: 1000 нейронов\n",
    "* Выходной слой: 10 нейронов (по одному на каждый класс)\n",
    "* Весов между входным и полносвязным слоями: (784 + 1)*1000 = 785.000 параметров\n",
    "* Весов между полносвязным и выходным слоями: (1000 + 1) * 10 = 10.010 параметров\n",
    "\n",
    "И как мы знаем, что если параметров больше чем данных - то это плохо.\n",
    "\n",
    "### Выводы\n",
    "\n",
    "Использование полносвязных нейронных сетей для работы с картинками является плохой идей так как:\n",
    "\n",
    "* Очень много параметров\n",
    "* Легко могут переобучиться\n",
    "* Не учитывают специфику изображений (сдвиги, небольшие изменения формы и т.д.)\n",
    "* Один из лучших способов борьбы с переобучением — снижение числа параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1498e34a",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">3 Свертки</h1>\n",
    "\n",
    "Эксперименты со зрительной корой https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1557912/\n",
    "\n",
    "### Процесс свертки\n",
    "\n",
    "**Шаг 1.**\n",
    "\n",
    "На входе имеется картинка размером 4х4 пикселя. Ещё у нас имеется некоторый фильтр (ядро свертки) - это некоторая картинка, но меньшего размера 2х2 пикслеля. Что такое свертка? Мы берем первый блок из основной картинки такого же размера как фильтр и умножаем его покоординатно на фильтр и суммируем что получилось.\n",
    "\n",
    "<img src='img/lecture02/15.png'>\n",
    "\n",
    "**Шаг 2.**\n",
    "\n",
    "Дальше берем следующий блок и так по всем блокам... Это и есть операция свертки. Фильтр мы будем обучать методом градиентного спуска. Фильтр одинаковый для всех частей картинки.\n",
    "\n",
    "<img src='img/lecture02/16.png'>\n",
    "\n",
    "Данную операцию можно применять для любых матриц и соответственно фильтр тоже может быть любого размера."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3bd116",
   "metadata": {},
   "source": [
    "### Смысл свертки\n",
    "\n",
    "Рассмотрим следующий фильтр. На главной диогонали расположены еденицы, а на побочной нули. Посмотрим как откликается фильтр на определенные картинки. Ну и ниже мы видим отклик 2, 2, 1, 0, 6, 10:\n",
    "\n",
    "<img src='img/lecture02/17.png'>\n",
    "\n",
    "Что это значит? Фильтр задает некоторый паттерн - что-то на диагонали, в нашем случае это черные пиксели. На первых двух откликах фильтра у нас действительно стоят 1 на диагонали. На отклике с выходом 1, черные пиксели стоят не везеде, а для отклика с ответом 0 черных пикселей вообще нет. Тем самым, отклик фильтра говорит о том, насколько паттерн присутствует на данному кусочке изображения.\n",
    "\n",
    "* Операция свёртки выявляет наличие на изображении паттерна, который задаётся фильтром\n",
    "\n",
    "* Чем сильнее на участке изображения представлен паттерн, тем больше будет значение свёртки\n",
    "\n",
    "Статья о свертках: https://arxiv.org/pdf/1603.07285.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea541ef",
   "metadata": {},
   "source": [
    "### Максимум свёртки инвариантен к сдвигам\n",
    "\n",
    "Если нас интересует вопрос, имеется ли на картинке где-то данный паттерн с диагональю. Имеется две картинки, на одной диагональ расположена в нижнем углу, а на второй в левом верхнем углу.\n",
    "\n",
    "<img src='img/lecture02/18.png'>\n",
    "\n",
    "Применив свертку мы получим некоторый результат, после возьмем максимум и на двух картинках мы получим результат равный 2. Если посмотреть, есть ли где-то 2, тогда мы проверим наличие паттерна. Это дает следующую идею, если мы захочем детектировать наличие еденицы, тогда возьмем некоторый фильтр, который определяет наличие еденицы под углом, после сделаем свертку и если у фильтра будет большой отклик, значит на картинке скорее всего есть еденица. Так как еденица может быть написана в разынх формах, тогда нам может понадобиться много разных фильтров.\n",
    "\n",
    "Значит, мы сможем находить, например еденицу, независимо от того, где она расположена, это хорошо демонстрирует пример выше, где мы взяли максимум двух сверток."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a2d86",
   "metadata": {},
   "source": [
    "### Свертки в компьютерном зрении\n",
    "\n",
    "**Пример 1**\n",
    "\n",
    "Ещё до появления нейронных сетей придумали, что использование фильтров это полезно. Например давно существует фильтр собеля, который позволяет находить горизонтальные переходы:\n",
    "\n",
    "<img src='img/lecture02/19.png'>\n",
    "\n",
    "https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1\n",
    "\n",
    "**Пример 2**\n",
    "\n",
    "Существует фильтр, который выполняет повышение резкости\n",
    "\n",
    "<img src='img/lecture02/20.png'>\n",
    "\n",
    "https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html\n",
    "\n",
    "**Пример 3**\n",
    "\n",
    "Фильтр размытие\n",
    "\n",
    "<img src='img/lecture02/21.png'>\n",
    "\n",
    "https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html\n",
    "\n",
    "А мы обсудим то, как фильтры автоматически подбирать под данные и объеденять в нейросетевые архитектуры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a1e56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
