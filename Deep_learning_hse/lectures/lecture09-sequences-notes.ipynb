{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af003887",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Представления для текстов</h1>\n",
    "\n",
    "## Bag of words\n",
    "\n",
    "* Заводим словарь, состоящий из всех слов в выборке\n",
    "* Делаем признак-индикатор для каждого слова из словаря\n",
    "* Можно добавлять n-граммы\n",
    "\n",
    "https://towardsdatascience.com/from-word-embeddings-to-pretrained-language-models-a-new-age-in-nlp-part-1-7ed0c7f3dfc5\n",
    "\n",
    "<img src='img/lecture09/1.png'>\n",
    "\n",
    "### Недостатки:\n",
    "\n",
    "* Слишком много признаков\n",
    "* Не учитываем смыслы слов\n",
    "* Семантически похожие тексты могут иметь очень разные представления"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe8af2",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">1. word2vec</h1>\n",
    "\n",
    "* Попробуем обучить вектор-представление для каждого слова\n",
    "* Что потребовать от такого представления?\n",
    "\n",
    "\n",
    "* Важная идея: если выкинуть слово, то оно должно хорошо восстанавливаться по представлениям соседних слов\n",
    "* Может применять и при работе с изображениями\n",
    "\n",
    "<img src='img/lecture09/2.png'>\n",
    "\n",
    "https://arxiv.org/abs/1301.3781\n",
    "\n",
    "# 1.1 Skip-gram model\n",
    "\n",
    "Вероятность встреить слово $w_O$ рядом со словом $w_I$:\n",
    "\n",
    "$$\\large P(w_O | w_I) = \n",
    "\\frac{\\exp (\\langle v_{w_O}^{'}, v_{w_I} \\rangle)}\n",
    "{\\sum\\limits_{w \\in W} \\exp(\\langle v_{w}^{'}, v_{w_I} \\rangle)}$$\n",
    "\n",
    "* $W$ - словарь\n",
    "\n",
    "Для каждого слова будем выучивать два вектора:\n",
    "\n",
    "* $v_w$ - \"Центральное\" представление слова. В случае, когда слово стоит в центре контекста и по нему предсказыаем соседние слова.\n",
    "* $v_{w}^{'}$ - \"Контекстное\" представление слова. Слово стоит вне контекста и его предсказываем по центральному слову.\n",
    "\n",
    "**Зачем выучивать два вектора?** Мы для каждого слова выучиваем два вектора, если слово является центральным и вектор, если слово стоит в контексте. Так работает лушче, можно было бы выучивать всего один вектор и использовать все без штрихов.\n",
    "\n",
    "### Разбор формулы:\n",
    "\n",
    "Cлово $w_O$ находится в контексте слова $w_I$, если между ними расстояние не больше некоторого чилса. Контекстом центарльного слова $w_I$ называется соседние $k$ слов (два слова до, два после).\n",
    "\n",
    "$P(w_O | w_I)$ - вероятность того, что встретим слово $w_O$ в контексте слова $w_I$. Ну эта вероятность считается по формле выше:\n",
    "\n",
    "1. В числителе берём скалярное произведение центального представления слова $w_I$ (для слова $w_I$ берём вектор, который предсказывает что рядом). Для слова $w_O$ берём контекстное представление. Получаем характеристику соноправленности двух векторов. Берём от этого экспоненту, так как скалярное произведение может быть отрицательным и положительными.\n",
    "\n",
    "2. И делим на сумму экспонент, где берём выходные векторы для всех слов. Это для нормировки, чтобы сумма всех вероятностей была равна 1, это просто SoftMax. Чем более соноправлены векторы двух слов, тем выще верояность того, что они встретятся вместе.\n",
    "\n",
    "При максимизации экспонента от скалярного произведения должна быть как можно больше, значит мы требуем чтобы само скалярное произведение было как можно больше. И при максимизации скалаярного произведения мы требуем соноправленности векторов. Если два слова встречаются рядом, то их векторы будут как можно более соноправлены."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951512a",
   "metadata": {},
   "source": [
    "### Функционал\n",
    "\n",
    "Функционал для текста $T = (w_1, w_2, \\ldots, w_n)$ из $n$-слов:\n",
    "\n",
    "$$\\large \\sum\\limits_{i=1}^{n} \\sum\\limits_{-c \\le j \\le c \\\\ \\quad j\\ne 0 \\\\ 1 \\le i + c \\le n} \\log P(w_{i + j} | w_j) \\to \\max$$\n",
    "\n",
    "1. Суммируем по всем словам в тексте, берём $i$-е слово.\n",
    "2. Суммируем по контексту, где $c$ - это размер контекста.\n",
    "3. Под суммой стоит вероятность встретить слово $w_{i +j}$ в контексте слова $w_i$, берём логарифм этой вероятности. \n",
    "\n",
    "Простоми словами, берём $i$ слово и требуем, чтобы вероятность встреить другие слова в его контексте была максимальна, только для слов из контекста.\n",
    "\n",
    "Максимизуерм по векторам слов $\\large v_{w_1}^{'}, \\ldots, v_{w_n}^{'}$ и по $\\large v_{w_1}, \\ldots, v_{w_n}$. Подбираем векторы слов так, чтобы вероятность встретить слово в его контексте была как можно больше.\n",
    "\n",
    "**Почему логарифм?** Мы записываем правдоподобие выборки, для этого небоходимо перемножать вероятности. А для того, чтобы мы их суммировали необходимо вязть логарифм."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf328af2",
   "metadata": {},
   "source": [
    "### Проблема формулы:\n",
    "\n",
    "$$\\large P(w_O | w_I) = \n",
    "\\frac{\\exp (\\langle v_{w_O}^{'}, v_{w_I} \\rangle)}\n",
    "{\\sum\\limits_{w \\in W} \\exp(\\langle v_{w}^{'}, v_{w_I} \\rangle)}$$\n",
    "\n",
    "* Считать знаменатель ОЧЕНЬ затратно\n",
    "* Значит, и производные считать тоже долго\n",
    "\n",
    "Необходимо избавиться от суммы, хочется чтобы вероятность не зависила от всего словаря."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ba71c",
   "metadata": {},
   "source": [
    "# 1.2 Negative sampling\n",
    "\n",
    "Заменим вероятность следующей формулой:\n",
    "\n",
    "$$\\large P(w_O | w_I) = \\log \\sigma (\\langle v_{w_O}^{'}, v_{w_I} \\rangle) + \\sum\\limits_{i=1}^k \\log \\sigma (-\\langle v_{w_i}^{'}, v_{w_I} \\rangle) \\to \\max$$\n",
    "\n",
    "* $w_i$ — случайно выбранные слова\n",
    "* Слово $w$ генерируется с вероятностью $P(w)$ — шумовое распределение\n",
    "* $\\large P(w) = \\frac{U(w)^{\\frac{3}{4}}}{\\sum\\limits_{v \\in W} U(v)^{\\frac{3}{4}}}$, $U(v)$ — частота слова $v$ в корпусе текстов\n",
    "\n",
    "\n",
    "1. Мы взяли два слова $w_I, w_O$. Берём сигмойду от скалярногоо произведения векторов двух слов и оно должно быть как можно больше. И это требование уже требует чтобы скалярное произвдение было как можно больше.\n",
    "\n",
    "В прошлой формуле, мы требовали, чтобы экспонента от скалярного произведения должна быть как можно больше. Если увеличивали скалярное произвдение для слова $w_O$ в контексте $w_I$, тогда уменьшались скалярные произвдения для $w_I$ со всеми другими словами, так как все вероятности суммируются в еденицу. Следовательно, когда мы настраиваем вероятность для одного слова, настраивались вероятности на всех других словах. \n",
    "\n",
    "И получается, только для тех слов, которые встречались рядом, вероятности большие. В Negative sampling мы потеряли данное свойство, мы просто требуем чтобы сигмойда от скалярного произведния была как можно больше. Добавим ещё одно слогаемое в функционал на который мы обучаем предсталвения:\n",
    "\n",
    "2. Сгенерируем для $w_I$ случайные слова из словаря и потребуем, чтобы для $w_i$ сигмойда от скалярного произведения была как можно меньше. Так как мы максимизируем, добавим минус.\n",
    "\n",
    "Тем самым мы избавились от нормировки, при этом будем минимизировать вероятности для из не контекста.\n",
    "\n",
    "### Особенности обучения:\n",
    "\n",
    "* Положительные примеры — слова, стоящие рядом\n",
    "*  Отрицательные примеры: подбираем к слову «шум», то есть другое слово, которое не находится рядом\n",
    "*  Важно семплировать в SGD слова с учётом их популярности — иначе будем обучаться только на самые частые слова\n",
    "\n",
    "### Как это использовать?\n",
    "\n",
    "* Можно искать похожие слова\n",
    "* Можно менять формы слов\n",
    "* Можно искать определённые отношения\n",
    "* Можно использовать как признаки для моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28432a76",
   "metadata": {},
   "source": [
    "# word2vec\n",
    "\n",
    "* Яркий пример self-supervision\n",
    "* Сейчас находит применения для изображения и даже для табличных данных\n",
    "* Оказывается, в данных очень много информации даже без разметки\n",
    "\n",
    "### Проблемы word2vec\n",
    "\n",
    "* Не учитываем структуру слов\n",
    "* Не закладываем никакой априорной информации о разных формах одного слова\n",
    "* Не умеем обрабатывать опечатки\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af3334",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">2. FastText</h1>\n",
    "\n",
    "& Заменим каждое слово на «мешок»\n",
    "* «руслан» $\\to$ (<руслан>, <ру, рус, усл, сла, лан, ан>)\n",
    "* Слово $\\large w$ заменяется на набор токенов $\\large t_1, \\ldots, t_n$\n",
    "* Мы обучаем векторы токенов: $\\large v_{t_1}, \\ldots, v_{t_n}$ (на самом деле есть «центральные» и «контекстные» версии всех векторов)\n",
    "* $\\large z_w = \\sum\\limits_{i=1}^n v_{t_i}$ — вектор слова складывается из подслов.\n",
    "* Все остальные детали — как в word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de4251",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Работа с текстом</h1>\n",
    "\n",
    "Научились строить векторы отдельных слов. Как это использовать для решения задач на текстах?\n",
    "\n",
    "* Векторные представления строятся для слов\n",
    "* Можно просто усреднить по всем словам — получим признаки для текста\n",
    "* Можно усреднять с весами\n",
    "* Можно ли умнее?\n",
    "\n",
    "<h1 style=\"color:#008B8B\">1. CNN для последовательностей</h1>\n",
    "\n",
    "Имеется некоторое предложение. Для каждого слова возьмём его вектор. Получаем матрицу число слов на размер векторых представлений.\n",
    " \n",
    "<img src='img/lecture09/3.png'>\n",
    "\n",
    "Дальше мы применяем свёртки. Берём фильр, размера два на число компонент вектора слов. И проходимся по всей матрице, получаем один канал. Полсе можно взять ещё один фильтр другого размера (по двум словама, трём...), получим ещё одни канал и так далее...\n",
    "\n",
    "При этом, свёртка должна быть полной по размеру векторного представления слов.\n",
    "\n",
    "Псоле берём каждый канал и записываем максимум, так как из-за того, что мы можем применять свёртки разных рамзеров, выходные каналы будут получаться тоже разных размеров. Полсе, получаем некоторый вектор, поверх которого добавляем полносвязные слои.\n",
    "\n",
    "### Инициализация\n",
    "\n",
    "<img src='img/lecture09/4.png'>\n",
    "\n",
    "* CNN-rand - случайно инициализируем векторы для кажого слова.\n",
    "* CNN-static - векторы из word2vec, без обучения.\n",
    "* CNN-non-static - векторы из word2vec c дообучения.\n",
    "\n",
    "### Недостатки\n",
    "\n",
    "* Ищем выразительные «локальные» комбинации\n",
    "* Не пытаемся понять смысл текста в целом"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
