{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6b546c",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Представления для текстов</h1>\n",
    "\n",
    "## Bag of words\n",
    "\n",
    "* Заводим словарь, состоящий из всех слов в выборке\n",
    "* Делаем признак-индикатор для каждого слова из словаря\n",
    "* Можно добавлять n-граммы\n",
    "\n",
    "https://towardsdatascience.com/from-word-embeddings-to-pretrained-language-models-a-new-age-in-nlp-part-1-7ed0c7f3dfc5\n",
    "\n",
    "<img src='img/lecture09/1.png'>\n",
    "\n",
    "### Недостатки:\n",
    "\n",
    "* Слишком много признаков\n",
    "* Не учитываем смыслы слов\n",
    "* Семантически похожие тексты могут иметь очень разные представления"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08972c40",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">1. word2vec</h1>\n",
    "\n",
    "* Попробуем обучить вектор-представление для каждого слова\n",
    "* Что потребовать от такого представления?\n",
    "\n",
    "\n",
    "* Важная идея: если выкинуть слово, то оно должно хорошо восстанавливаться по представлениям соседних слов\n",
    "* Может применять и при работе с изображениями\n",
    "\n",
    "<img src='img/lecture09/2.png'>\n",
    "\n",
    "https://arxiv.org/abs/1301.3781\n",
    "\n",
    "## Skip-gram model\n",
    "\n",
    "Вероятность встреить слово $w_O$ рядом со словом $w_I$:\n",
    "\n",
    "$$\\large P(w_O | w_I) = \n",
    "\\frac{\\exp (\\langle v_{w_O}^{'}, v_{w_I} \\rangle)}\n",
    "{\\sum\\limits_{w \\in W} \\exp(\\langle v_{w}^{'}, v_{w_I} \\rangle)}$$\n",
    "\n",
    "* $W$ - словарь\n",
    "\n",
    "Для каждого слова будем выучивать два вектора:\n",
    "\n",
    "* $v_w$ - \"Центральное\" представление слова. В случае, когда слово стоит в центре контекста и по нему предсказыаем соседние слова.\n",
    "* $v_{w}^{'}$ - \"Контекстное\" представление слова. Слово стоит вне контекста и его предсказываем по центральному слову.\n",
    "\n",
    "**Зачем выучивать два вектора?** Мы для каждого слова выучиваем два вектора, если слово является центральным и вектор, если слово стоит в контексте. Так работает лушче, можно было бы выучивать всего один вектор и использовать все без штрихов.\n",
    "\n",
    "### Разбор формулы:\n",
    "\n",
    "Cлово $w_O$ находится в контексте слова $w_I$, если между ними расстояние не больше некоторого чилса. Контекстом центарльного слова $w_I$ называется соседние $k$ слов (два слова до, два после).\n",
    "\n",
    "$P(w_O | w_I)$ - вероятность того, что встретим слово $w_O$ в контексте слова $w_I$. Ну эта вероятность считается по формле выше:\n",
    "\n",
    "* В числителе берём скалярное произведение центального представления слова $w_I$ (для слова $w_I$ берём вектор, который предсказывает что рядом). Для слова $w_O$ берём контекстное представление. Получаем характеристику соноправленности двух векторов. Берём от этого экспоненту, так как скалярное произведение может быть отрицательным и положительными.\n",
    "\n",
    "* И делим на сумму экспонент, где берём выходные векторы для всех слов. Это для нормировки, чтобы сумма всех вероятностей была равна 1, это просто SoftMax. Чем более соноправлены векторы двух слов, тем выще верояность того, что они встретятся вместе.\n",
    "\n",
    "При максимизации экспонента от скалярного произведения должна быть как можно больше, значит мы требуем чтобы само скалярное произведение было как можно больше. И при максимизации скалаярного произведения мы требуем соноправленности векторов. Если два слова встречаются рядом, то их векторы будут как можно более соноправлены."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd98f8",
   "metadata": {},
   "source": [
    "### Функционал\n",
    "\n",
    "Функционал для текста $T = (w_1, w_2, \\ldots, w_n)$ из $n$-слов:\n",
    "\n",
    "$$\\large \\sum\\limits_{i=1}^{n} \\sum\\limits_{-c \\le j \\le c \\\\ \\quad j\\ne 0 \\\\ 1 \\le i + c \\le n} \\log P(w_{i + j} | w_j) \\to \\max$$\n",
    "\n",
    "* Суммируем по всем словам в тексте, берём $i$-е слово.\n",
    "* Суммируем по контексту, где $c$ - это размер контекста.\n",
    "* Под суммой стоит вероятность встретить слово $w_{i +j}$ в контексте слова $w_i$, берём логарифм этой вероятности. \n",
    "\n",
    "Простоми словами, берём $i$ слово и требуем, чтобы вероятность встреить другие слова в его контексте была максимальна, только для слов из контекста.\n",
    "\n",
    "Максимизуерм по векторам слов $\\large v_{w_1}^{'}, \\ldots, v_{w_n}^{'}$ и по $\\large v_{w_1}, \\ldots, v_{w_n}$. Подбираем векторы слов так, чтобы вероятность встретить слово в его контексте была как можно больше.\n",
    "\n",
    "**Почему логарифм?** Мы записываем правдоподобие выборки, для этого небоходимо перемножать вероятности. А для того, чтобы мы их суммировали необходимо вязть логарифм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c412175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
