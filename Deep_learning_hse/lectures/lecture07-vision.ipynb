{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab71f9cf",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Интерпретация моделей</h1>\n",
    "\n",
    "<h1 style=\"color:#008B8B\">1. Что находят свёрточные сети?</h1>\n",
    "\n",
    "https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf\n",
    "\n",
    "* Можно для каждого фильтра найти кусочки картинок, дающие самый сильный отклик\n",
    "* Сделаем это для AlexNet\n",
    "\n",
    "Можно среди всей обучающей выборки найти картинку и кусочек на ней, где фильтр дает наибольший отклик. Посомтрим на пример 3-го сверточного слоя:\n",
    "\n",
    "<img src='img/lecture07/1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea524e6",
   "metadata": {},
   "source": [
    "## Максимизация вероятности класса\n",
    "\n",
    "### Вариант 1\n",
    "\n",
    "https://arxiv.org/pdf/1312.6034.pdf\n",
    "\n",
    "Возьмем обученную нейронную сеть $a_y(x)$, где $y$ - это выход конкретного класса, например, вероятность того, что это кошка. Дальше подаем на вход случайную картинку $x$, после чего начинаем изменять данную картинку градиентным спуском, чтобы максимизировать вероятность того, что на картинке кошка. Мы же можем градиентным спуском обучать не параметры, а саму картинку. Тогда посчитам backprop выходы сетки по пикселям входной картинки, после чего сдвигать пиксели, чтобы вероятность максимизировалась.\n",
    "\n",
    "Так же, имеется регуляризатор, который ограничивает величины интенсивности цветов, которые записаны в пикселях.\n",
    "\n",
    "$\\large a_y(x) - \\lambda \\|x\\|_2^2 \\to \\underset{x}{\\text{min}}$\n",
    "\n",
    "Посмотрим, какие картинки с точки зрения нейронной сети с большей вероятностью содержат тот или иной класс:\n",
    "\n",
    "<img src='img/lecture07/2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd4d2f",
   "metadata": {},
   "source": [
    "### Вариант 2\n",
    "\n",
    "Здесь улучшенная версия\n",
    "\n",
    "https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html\n",
    "\n",
    "<img src='img/lecture07/3.png'>\n",
    "\n",
    "Давайте теперь инициализировть вход не случайным шумом, как это было на прошлых картинках, а возьмем за основу некоторую картинку и будем максимизировать вероятность некоторого класса.\n",
    "\n",
    "<img src='img/lecture07/4.png'>\n",
    "\n",
    "Дальше можно взять большие картинки и максимизировать вероятность некоторого класса.\n",
    "\n",
    "<img src='img/lecture07/5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4abdb82",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Компьютерное зрение - Сегментация</h1>\n",
    "\n",
    "Мы научились выполнять классификацию изображений с использованием нейронных сетей.\n",
    "\n",
    "<img src='img/lecture07/6.png'>\n",
    "\n",
    "**Зачастую, хочется решать другие задачи:**\n",
    "\n",
    "* Semantic Segmentation - какой пиксель картинки относится к какому классу. Желтый пиксел - это класс кот, фиоллетовый пиксель - это лес...\n",
    "\n",
    "* Object detection - не просто говорим что собака находится на изображении, а говорим где она находится, выделив их прямоугольниками.\n",
    "\n",
    "* Instanct Segmentation - в этом примере мы выделяем область на картинге, где имеется собака."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25873d96",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">1. Семантическая сегментация</h1>\n",
    "\n",
    "Все задачи выше мы будем сводить к классификации, так как мы уже умеем решать данную задачу.\n",
    "\n",
    "<img src='img/lecture07/7.png'>\n",
    "\n",
    "В этой задаче необходимо для каждого пикселя картинки сказать, к какому классу этот пиксель относится.\n",
    "\n",
    "<h2 style=\"color:#008B8B\">1.1 Постановка задачи</h2>\n",
    "\n",
    "* Данные: изображения и их корректные сегментации\n",
    "* Пример: PASCAL VOC 2012\n",
    "\n",
    "Изначально, необходимо разобраться как устроены данные.\n",
    "\n",
    "<img src='img/lecture07/11.png'>\n",
    "\n",
    "Как видим, у нас имеется весьма мало объектов внутри классов, как мы будем это обучать на таком маленьком наборе данных? Поскольку каждая картинка несет в себе дополнительную информацию, так как класс известен для каждого пикслея. В случае, если у нас $1500$ картинок размером $100 \\times 100$, тогда всего пикселей, для которых известны классы $15,000, 000$. \n",
    "\n",
    "Следовательно, в случае сегментации мы можем работать с очень маленькими выбороками, так как мы решаем задачу классификации не картинок, а классификации каждого пикселя.\n",
    "\n",
    "* Каждый объект содержит сильно больше информации, чем при\n",
    "классификации!\n",
    "* Можно хорошо обучаться на небольших выборках"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86652439",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">1.2 Метрики качества</h2>\n",
    "\n",
    "### Попиксельная доля верных ответов:\n",
    "\n",
    "$$\\large L(y, a) = \\frac{1}{n} \\sum\\limits_{i=1}^n [y_i = a_i]$$\n",
    "\n",
    "Где $n$ - число пикселей в картинке. $y_i$ правильный класс для $i$-го пикслея, $a_i$ класс, который выдала модель для $i$-го пикселя.\n",
    "\n",
    "Суммируем по всем пикселям и смотрим долю пикселей где модель угадала правильный класс. Обучаться модель будет на другой функционал, так как данный функционал не дифференцируемый.\n",
    "\n",
    "### Мера Жаккара (считается отдельно для каждого класса):\n",
    "\n",
    "Если почти всю картинку занимает автобус, но модель захватила весь автобус и несколько областей рядом. Скорее всего в этом нет ничего страшного, так как автобус был найден и мы не хотим штрафовать модель за небольшие отклонения от правильного сегмента. Для этого используется Мера Жаккара (IoU): \n",
    "\n",
    "$$\\large J_k (y, a) = \\frac{\\sum_{i=1}^n [y_i = k] [a_i = k]}{\\sum_{i=1}^n \\text{max}([y_i = k], [a_i = k])}$$\n",
    "\n",
    "\n",
    "Мера Жаккара считается для каждого класса в отдельности. Например, мы взяли калсс автобус и проходимся по всем пикселям. В числителе считаем число пикселей, которые должны относиться к автобусу и модель отнесла их к автобусу. В знаменателе считаем число пикселей, которые на самом деле относятся к автобусу или модель отнесла к автобусу.\n",
    "\n",
    "Другими словами, мы берем две области. Первая область, где модель отнесла к автобусу и где нас самом деле находится автобус. В числителе мы считаем площадь пересечения двух областей и делим на площадь объеденения двух областей. Можно посчитать меру Жаккара для каждого класса и усреднить по всем классам."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e61a3",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#008B8B\">1.3 Функция потерь</h2>\n",
    "\n",
    "### Для одного изображения (categorical cross-entropy, CCE):\n",
    "\n",
    "<img src='img/lecture07/12.png'>\n",
    "\n",
    "Просуммируем по всем пикселям картинки, в каждом пикселе просуммируем по всем классам (например, 20 различных классов, автобус, машина...). Берем идикатор того, что $y_i$ пиксель относится к $k$ классу (так как пиксель может относится только к одному классу) и умножаем на логарифм $a_{ik}$ вероятности $k$-го класса на $i$ пикселе. \n",
    "\n",
    "И все это мы будем максимизировать, тем самым требуя, чтобы вероятность правильного класса в каждом пикселе была как можно больше, значит и хотим, чтобы распределение вероятностей было вырожденным. Можно поставить минус перед функцией потерь и минимизировать функцию потерь. В логистической регрессии была подобная запись, только здесь записано для случия многих классов. \n",
    "\n",
    "$a_{ik}$ - Это выход модели в $i$-ом пикселе для $k$-го класса. Мы считаем, что модель $a(x)$ принимает на вход картинку и если исходная картинка $n \\times m \\times c$, тогда модель вернет картинку $n \\times m \\times K$. Сдовательно, на выходе модели записана вероятность каждого из классов. Мы считаем, что модель должна для каждого пикселя выдать $K$ вероятностей (распределение классов в этих точках).\n",
    "\n",
    "**Почему берем логарифм?** Вспомним проблему сигмойды, что происходит затухание градиента. Засчет того, что мы используем логарифм, решается проблема насыщения. Поскольку $a_{ik}$ - это softmax (может происходить насщение), логарифмом мы получаем функцию, которая решает эту пробему. Ну и при этом, можно интерпретировать данную запись как правдоподобие... \n",
    "\n",
    "### SoftMax\n",
    "\n",
    "Возникает вопрос, как получить вероятности? Имеется некоторая архитектура нейронной сети, которая, например, после полносвязного слоя для каждого пикселя выдает вектор из $K$ чисел. А нам хочется получать вероятности, чтобы они суммировались в еденицу и чтобы они все были неотрицательны. И набор из $K$ числе превратить в набор вероятностпй можно использованием softmax.\n",
    "\n",
    "Если модель в $i$-м пикселе выдаёт какие-то числа $b_{i1}, \\ldots, b_{iK}$, то их можно превратить в вероятности через softmax:\n",
    "\n",
    "$$\\large a_{ik} = \\frac{\\exp(b_{ik})}{\\sum_{m=1}^k \\exp(b_{im})}$$\n",
    "\n",
    "Например, для какого пикслея на выходе получилось $K$ числе на выходе: $b_{i1}, \\ldots, b_{iK}$, тогда возьмем от каждого из чисел экспоненту и разделим на сумму экспонент.\n",
    "\n",
    "Важно, что softmax просто преобразует выходы. А вероятности получаются засчет того, что мы обучаем на подобранный функционал."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c1bb2",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">2. Fully Convolutional Network</h1>\n",
    "\n",
    "Fully Convolutional Network - означает, что в нейронной сети нет полносвязных слоев, в ней только сверточные слои, max pooling.\n",
    "\n",
    "https://arxiv.org/abs/1411.4038\n",
    "\n",
    "Начнем с первого подхода к сегментации изображений. Имеется картинка слева и мы хотим получить карту сегментации справа, где для каждого пикселя сказано к какому классу он относится.\n",
    "\n",
    "Возьмем сверточную нейронную сеть VGG/AlexNet... мы помним, что данные архитектуры представляют собой последовательность светочных слоев с последующими полносвязными слоями на концах. В архитектуре оставим только сверточную часть нейронной сети убрав полносвязные слои. В конце такой архитектуры после последнего сверточного слоя на выходе получается некоторый тензор небольшого размера (определнной ширины и высоты) с 21 каналом. Если изначально мы начинали с картинки размером $200 \\times 200 \\times 3$, то в конце получается картинка, например, размера $20 \\times 20 \\times 21$ и эту картинку необходимо расширить в картинку исходного размера с 21 каналом ($200 \\times 200 \\times 21$). И к этому применяем полносвязный слой, чтобы превратить полученные каналы в вероятности классов для каждого пикселя.\n",
    "\n",
    "Замечание, 21 канал получился не просто так. Так как мы делаем количество каналов в последенм сверточном слое столько, сколько классов. \n",
    "\n",
    "<img src='img/lecture07/8.png'>\n",
    "\n",
    "**Итого:**\n",
    "\n",
    "* Убираем полносвязные слои\n",
    "* Остаются только свёртки\n",
    "* Тензор с последнего слоя преобразуем свёртками 1x1 в тензор такого же размера, но с $K$ каналами (по числу классов)\n",
    "* Повышаем разрешение\n",
    "    * Можно простым размножением пикселей\n",
    "    * Можно хитрее\n",
    "    \n",
    "### Пример работы\n",
    "\n",
    "FCN-xxs - число характеризует архитектуру, чем меньше число, тем больше архитектура. Как видим даже самая большая архитектура плохо работает. \n",
    "\n",
    "<img src='img/lecture07/9.png'>\n",
    "\n",
    "**Чем это плохо?**\n",
    "\n",
    "* Тензор на последнем слое довольно маленький\n",
    "* Классы предсказываются грубо, у объектов нечёткие границы\n",
    "\n",
    "Почему это работает плохо? Так как мы сжали огромную картинку путем сверток и max pooling до небольшого размера, потеряв много информации. У нас информации мало, она сжатая и обобщенная, так как мы потеряли знание об отдельных пикселях. А после мы все это разворачиваем в большую картинку и пытаемся классифицировать пиксели.\n",
    "\n",
    "**Замечание:** В нашей архитектуре у выхода последнего сверточного слоя с 21 каналом большое поле восприятие, следовательно каждый фильтр видит всю картинку и мы выполняем сегментацию опираясь на полную информацию о картинке. Если взять только первый сверточный слой, который имеет большой размер и по нему построим сегментацию, тогда это будет полохо, так как у каждого фильтра маленькое поле восприятия. Следовательно, мы принимаем решение о том, к какому классу относится пикслеь только на основе небольшой области, что плохо.\n",
    "\n",
    "Делаем вывод, что нам необходимо балансироваться между полем восприятия и размером картинки, что весьма сложно. Поэтому придумали другие архитектуры, которые мы рассмотрим ниже.\n",
    "\n",
    "**Решение:**\n",
    "\n",
    "* Можно делать меньше пулингов\n",
    "* Но тогда будут проблемы с размером поля восприятия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396fb44a",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">3. U-Net</h1>\n",
    "\n",
    "https://arxiv.org/abs/1505.04597\n",
    "\n",
    "**Шаг 1.** Имеется входная картинка размером $572 \\times 572 \\times 1$ и выполняем две операции свертки с фильтром размера $3 \\times 3$ с последующим применением ReLU после каждой свертки.  Заметим, что после сверток изображение уменьшается в размере, так как не используется Padding.\n",
    "\n",
    "**Шаг 2.** Используем max pool $2 \\times 2$ и повторяем операцию свертки. И так несколько раз...\n",
    "\n",
    "**Шаг 3.** Получаем после очередного max pooling картинку размером $32 \\times 32 \\times 512$. После выполняем две операции всертки подряд и на выходе получаем изображение небольшого размера $28 \\times 28 \\times 1024$\n",
    "\n",
    "<img src='img/lecture07/13.png'>\n",
    "\n",
    "**Шаг 4.** Применяем операцию up-conv $2 \\times 2$ которая увеличивает входное изображение $28 \\times 28 \\times 1024$ (Тензор 1) в два раза некоторым образом и подгоняем количество каналов как в левой ветке (Зеленая стрелочка). После преобразований получаем картинку размером $56 \\times 56 \\times 512$ (Тензор 2). Тензор изображений из левой ветки размером $64 \\times 64 \\times 512$ обрезаем до $56 \\times 56 \\times 512$, после эту преобразованную картинку из левой ветки добавяем в качестве каналов к основной картинке (3 тензор).\n",
    "\n",
    "**Шаг 5.** Выполняем такую опреацию несколько раз.\n",
    "\n",
    "**Шаг 6.** На последней серой стрелочке мы берем исходную картинку без использования пулинга, соответственно мы имеем исходную информацю о каждом пикселе. Применяем два раза свертки и пследнюю свертку с фильтром $1 \\times 1$, которая сжимает все каналы в такокое количество - сколько у нас классов. \n",
    "\n",
    "После получаем карту сегментации, которая представляет собой картинку исходного размера, где для каждого пикселя сказано, к какому классу он относится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5661f16e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
