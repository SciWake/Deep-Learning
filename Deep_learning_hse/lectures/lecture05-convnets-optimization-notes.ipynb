{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe96cea",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black\" align=\"center\">Нормализации</h1>\n",
    "\n",
    "<h1 style=\"color:#008B8B\">1. Идея нормализации</h1>\n",
    "\n",
    "### Covariate shift\n",
    "\n",
    "В машинном обучении существует термин - Covariate shift, который обозначает сдвиг области определения. Имеется некоторая обучающая выборка с двумя классами, которая устроена следующим образом:\n",
    "\n",
    "<img src='img/lecture05/1.png' width=\"500\" height=\"500\">\n",
    "\n",
    "Исходный набор данных модель способна отделить прямой синего цвета. После, на тестовой выборке, которую мы собрали позже, распределение признаков изменилось (немного съехали под другим углом). Тогда, синяя модель будет плохо работать на тестовых данных и тут будет хорошей моделью красная линия.\n",
    "\n",
    "* В классическом машинном обучении — изменение распределения данных\n",
    "* Много методов решения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38eb6c1",
   "metadata": {},
   "source": [
    "### Domain adaptation\n",
    "\n",
    "Имеется обучающая и тестовая выборка. Предположим у нас имеются тестовые объекты, но мы не имеем значение целевой переменной. \n",
    "\n",
    "* Объекты по-разному распределены на обучении и на контроле\n",
    "* Идея: взвешивать объекты при обучении\n",
    "\n",
    "Тогда при обучении в функционал вставим веса $s_i$ при каждом объекте $x_i$ и этот вес будет большим, если $x_i$ объект похож на объект из тестовой выборки, а если объект не похож на тестовый, тогда вес будет маленьким:\n",
    "\n",
    "$$\\large \\sum\\limits_{i=1}^{\\ell} s_i (a(x_i) - y_i)^2 \\to \\underset{a(x)}{\\text{min}} $$\n",
    "\n",
    "* Большие веса будем ставить объектам, которые похожи на объекты из тестовой выборки\n",
    "\n",
    "Да, при обучении мы смотрим на тестовоую выборку, но мы используем только признаки тестовой выборки не используя целевую переменную."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d7b89",
   "metadata": {},
   "source": [
    "### Internal covariate shift\n",
    "\n",
    "* В нейронной сети каждый слой обучается на выходах предыдущих слоёв\n",
    "* Если слой в начале сильно меняется, то все следующие слои надо переделывать\n",
    "\n",
    "Имеется некоторая нейронная сеть:\n",
    "\n",
    "<img src='img/lecture05/2.png' width=\"500\" height=\"500\">\n",
    "\n",
    "Предположим, что градиентный спуск в процессе и мы находимся на 100-й итерации (одна итерация - проход по батчу), следовательно, мы имеем уже некоторые параметры. \n",
    "\n",
    "Cейчас, на выходе первого полносвязно слоя имеются числа (1, 2, 0, 1) и веса при следующем слое еденичные. Предположим, мы делаем следующий градиентный шаг и в результате очередной итерации градиентного спуска - сильно изменились веса первого полносвязного слоя. Из-за этого сильно изменились выходы первого полносвязного слоя:\n",
    "\n",
    "<img src='img/lecture05/3.png' width=\"500\" height=\"500\">\n",
    "\n",
    "Из-за такого изменения последующие слои потеряли смысл, так как они приспособлены к другому масштабу около еденицы. Последующие слои обучались на более меньший масштаб и из-за данного изменения последующие слои будут плохо работать. Следовательно, если сильно меняется изначальный слой, тогда все последующие необходимо перестраивать.\n",
    "\n",
    "* Идея: преобразовывать выходы слоёв так, чтобы они гарантированно имели фиксированное распределение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2aa59a",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#008B8B\">2. Batch Normalization</h1>\n",
    "\n",
    "* Реализуется как отдельный слой\n",
    "* Вычисляется для текущего батча\n",
    "\n",
    "Нормализация вставляется как ещё один слой. Имеется некоторый $FC$ слой, выходы которго подаются на вход Batch Norm Layer и он выдает вектор такой же длины как входной.\n",
    "\n",
    "Рассмотрим подробнее как это работает? Мы обучаем модель батчами, имеется некоторая начальная сеть, через которую мы прогоняем батч и на выходе получаем для каждого объекта свой вектор. Эти выходы мы подаем на вход батч-норм-слою, который по каждой компоненте считает статистики. Если например, векторы размера 200, тогда и $\\mu_B$ будет размера 200.\n",
    "\n",
    "* Оценим среднее и дисперсию каждой компоненты входного вектора:\n",
    "\n",
    "$$\\large \\mu_{B} = \\frac{1}{n} \\sum\\limits_{j=1}^{n} x_{B, j}\n",
    "\\qquad\n",
    "\\sigma_{B}^2 = \\frac{1}{n} \\sum\\limits_{j=1}^{n} (x_{B, j} - \\mu_{B})^2$$\n",
    "\n",
    "Где $x_{B,j}$ - $j$-й объект в батче $B$\n",
    "\n",
    "Следовательно мы получам вектор среднего и вектор дисперсий. \n",
    "\n",
    "* Отмасштабируем все выходы:\n",
    "\n",
    "$$\\large \\tilde{x}_{B, j} = \\frac{x_{B,j} - \\mu_{B}}{\\sqrt{\\sigma^2_{B} + \\epsilon}}$$\n",
    "\n",
    "Это ограничивает возможности нейронной сети, так как мы требуем, что после каждого слоя, каждый нейрон имеет нулевое среднее и еденичное стандартное отклонение. \n",
    "\n",
    "Поэтому, добавим парамтров в нейронную сеть. После того, как мы выполнили стандартизацию, у каждого $j$-го выхода предыдущего слоя среднее $\\beta$ и стандартное отклонение $\\gamma$. Для этого зададим нужные нам среднее и дисперсию следующим образом:\n",
    "\n",
    "$$\\large z_{B, j} = \\gamma_j \\circ \\tilde{x}_{B,j} + \\beta_j$$\n",
    "\n",
    "После этого, получится что $j$-й выход имеет среднее $\\beta$ и стандартное отклонение $\\gamma$, где $\\beta, \\gamma$ - это обучаемы параметры.\n",
    "\n",
    "**Итого:** Батч-норм-слой гарантирует, что после прохождения батча через него, по каждой координате свое фиксированное среднее и дисперсия.\n",
    "\n",
    "**Этап применения:**\n",
    "\n",
    "Во время применения нейронной сети:\n",
    "* Те же самые формулы, но вместо $\\mu_{B}$ и $\\sigma_{B}^2$ используем их средние значения по всем батчам, так как нейронная сеть применяется к отедльным объектам, а не к батчам.\n",
    "\n",
    "**Как использовать?**\n",
    "\n",
    "* Обычно вставляется между полносвязным/свёрточным слоём и нелинейностью\n",
    "* Использование Batch Normalization позволяет увеличить длину шага в градиентном спуске, что ускоряет обучение.\n",
    "* Не факт, что действительно устраняет covariance shift. Но если наши предположения были не верны и мы не решим проблему covariance shift, тогда использовать Batch Normalization стоит, так как эксперементы показывают, что Batch Normalization улучшает качество обучения нейронной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb5b3b",
   "metadata": {},
   "source": [
    "## 2.1 В чём польза от BatchNorm?\n",
    "\n",
    "Статья была устроена так, выдвинули гипотезу, проверили - работает! Но соврешенно не понятно, верна ли гипотеза? Верно ли что слои могу резко измениться, верны ли все домыслы? \n",
    "\n",
    "После выхода статьи про Batch Normalization, вышла новая статья под названием $\\text{How does batch normalization help optimiztion?}$. В статье провели ряд эксерементов. \n",
    "\n",
    "### Гипотеза 1\n",
    "\n",
    "Если Batch Normalization помогает стабилизировать распределение выходов, спасая от резких изенений (за счет того, что распределение выходов не изменится) из-за которых необходимо престраивать все последующие веса нейронной сети. \n",
    "\n",
    "Тогда, возьмем нейронную сеть, которая хорошо обучается с использованием BatchNorm. Известно, что BatchNorm выдает векторы со стабильным распределением, известным средним и дисперсией. Добавим шума вектор, который выдет BatchNorm, испортив тем самым распределение выхода BatchNorm после каждого шага.\n",
    "\n",
    "<img src='img/lecture05/6.png' width=\"700\" height=\"700\">\n",
    "\n",
    "Оказывается, это ничего не портит, нейронная сеть все равно хорошо обучается, аналогично тому, как обучение без добавления шума. Тогда, логично, что BatchNorm не связан с выравниванием распределения слоёв. \n",
    "\n",
    "### Гипотеза 2\n",
    "\n",
    "Была проблема с градиентами. Если веса в начале нейронной сети изменяются сильно, тогда придется сильно перестраиваться веса при следующих слоях. А мы выдвинули гипотезу, что BatchNorm должен исправлять эту проблему. И если провести сравнение градиентов по разным слоям между итерациями, окажется, что при использовании BatchNorm градиенты так же сильно изменяются: \n",
    "\n",
    "<img src='img/lecture05/7.png' width=\"700\" height=\"700\">\n",
    "\n",
    "Следовательно, когда используется BatchNorm, иногда, в процессе обучении сильно изменяются веса во всех слоях. Делаем выводы, что BatchNorm не решает проблему сильного перестраивания дальнейших слоев при изменении ранних слоев.\n",
    "\n",
    "### Польза от BatchNorm\n",
    "\n",
    "Оказывается, что при добавлении BatchNorm в нейронную сеть делат процесс оптимизации более простой, за счет того, что функционал ошибки становится более гладким. И после добавления BatchNorm обучение становится более стабильным:\n",
    "\n",
    "<img src='img/lecture05/8.png' width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed808922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1be50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
